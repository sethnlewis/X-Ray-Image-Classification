{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The purpose of this analysis is to design a model that can use chest x-ray images to identify pneumonia. According to the American Thoracic Society, pneumonia is the [leading cause of death](https://www.thoracic.org/patients/patient-resources/resources/top-pneumonia-facts.pdf) among children under age 5, accounting for roughly 16% of all deaths within that age range in 2015. The application of machine learning techniques such as neural networks can help identify the presence of pneumonia using exclusively chest x-rays. \n",
    "\n",
    "\n",
    "### Data Sources\n",
    "The data used in this analysis was original provided by Mendeley Data and is publicly available [here](https://data.mendeley.com/datasets/rscbjbr9sj/3) (1). The dataset was subsequently adapted to a Kaggle dataset, which can be found [here](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia).\n",
    "\n",
    "In total, 5856 X-ray images in jpeg format are provided. Each is already labeled to indicate whether or not pneumonia is present.\n",
    "\n",
    "### The Process\n",
    "\n",
    "This analysis will follow the general structure listed here:\n",
    "1. Import packages and data\n",
    "2. Function definitions\n",
    "3. Modeling\n",
    "4. Evaluation\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Import packages and data\n",
    "\n",
    "##### Package Imports\n",
    "The below packages are necessary for various functionality throughout the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:02:42.344583Z",
     "iopub.status.busy": "2021-07-02T23:02:42.344256Z",
     "iopub.status.idle": "2021-07-02T23:02:42.353289Z",
     "shell.execute_reply": "2021-07-02T23:02:42.352429Z",
     "shell.execute_reply.started": "2021-07-02T23:02:42.344552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys, os\n",
    "\n",
    "# Use Scikit-learn for train-test splits and model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Import various Keras/Tensorflow packages for neural networks\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import metrics\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, array_to_img\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "# Global static variable declarations\n",
    "RANDOM_STATE = 2020 # Ensure repeatable results\n",
    "VAL_SPLIT = 0.25 # Use 25% of data for validation and test splits\n",
    "\n",
    "# Set resolution of each photo. Each image is downsampled to this size (in pixels)\n",
    "TARGET_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Operating platform selection\n",
    "Since this project has been run using local CPUs in addition to the GPUs offered by Google Colab and Kaggle Notebooks, the below cell quickly enables to user to select the platform of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:02:42.374840Z",
     "iopub.status.busy": "2021-07-02T23:02:42.374458Z",
     "iopub.status.idle": "2021-07-02T23:02:42.380604Z",
     "shell.execute_reply": "2021-07-02T23:02:42.379576Z",
     "shell.execute_reply.started": "2021-07-02T23:02:42.374811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set 'platform' to any of the following: \"kaggle\", \"colab\", \"local\" depending on use case\n",
    "platform = 'local'\n",
    "\n",
    "def set_data_path(platform):\n",
    "    \n",
    "    # Set file paths if running on Kaggle notebook\n",
    "    if platform == 'kaggle':\n",
    "        path_train = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/train/'\n",
    "        path_val   = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/'\n",
    "        path_test  = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/test/'\n",
    "    \n",
    "    # Set file paths if running on Google Colab\n",
    "    elif platform == 'colab':\n",
    "        path_train = '/content/drive/MyDrive/Data Science/Colab Notebooks/Module 4 Project/data/raw/train'\n",
    "        path_val   = '/content/drive/MyDrive/Data Science/Colab Notebooks/Module 4 Project/data/raw/val'\n",
    "        path_test  = '/content/drive/MyDrive/Data Science/Colab Notebooks/Module 4 Project/data/raw/test'\n",
    "        \n",
    "        # Notebook must be \"mounted\" to drive to access stored data files\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "    # Set file paths if running locally\n",
    "    elif platform == 'local':\n",
    "        path_train = '../data/raw/train'\n",
    "        path_val = '../data/raw/val'\n",
    "        path_test  = '../data/raw/test'\n",
    "\n",
    "    else:\n",
    "        sys.exit('ERROR: PLEASE ENTER \"kaggle\", \"colab\" or \"local\"')\n",
    "    \n",
    "    return path_train, path_val, path_test\n",
    "\n",
    "\n",
    "# Run function to determine the data paths for the platform being used\n",
    "path_train, path_val, path_test = set_data_path(platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Imports\n",
    "It has been made clear through external research that substantial data within the provided Test dataset has been mislabeled. As a result, a model cannot effectively predict correctly when trained on correctly labeled data. To address the issue, data from both Train and Test folders are imported, combined, then randomly split within this notebook. This minimizes the impact of mislabeled data and in turn creates a model that is more generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create function to import all images from provided directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:02:42.387547Z",
     "iopub.status.busy": "2021-07-02T23:02:42.387089Z",
     "iopub.status.idle": "2021-07-02T23:02:42.394755Z",
     "shell.execute_reply": "2021-07-02T23:02:42.393974Z",
     "shell.execute_reply.started": "2021-07-02T23:02:42.387513Z"
    }
   },
   "outputs": [],
   "source": [
    "def import_data(path, file_ct, resolution=TARGET_SIZE):\n",
    "    # Batch size is defined such that *all* images in selected folder are imported in one batch\n",
    "        \n",
    "        \n",
    "    # Instantiate Keras generator, scaling all RGB inputs from the \n",
    "    # default [0, 255] range to the [0, 1] range since neural network inputs should be \n",
    "    # normalized\n",
    "    generator = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "    # Create flow object to bring images from folders to memory\n",
    "    gen_train = generator.flow_from_directory(directory=path, \n",
    "                                              target_size=(resolution, resolution), \n",
    "                                              batch_size=file_ct, \n",
    "                                              seed=RANDOM_STATE)\n",
    "    # Store all images in numpy array\n",
    "    data_and_labels = next(gen_train)\n",
    "\n",
    "    \n",
    "    return data_and_labels[0], data_and_labels[1] # images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the above-defined function\n",
    "Create datasets and aggregate them into one final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:02:42.410044Z",
     "iopub.status.busy": "2021-07-02T23:02:42.409807Z",
     "iopub.status.idle": "2021-07-02T23:03:34.054858Z",
     "shell.execute_reply": "2021-07-02T23:03:34.053046Z",
     "shell.execute_reply.started": "2021-07-02T23:02:42.410022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n",
      "Image data shape: (5856, 256, 256, 3)\n",
      "Label data shape: (5856, 2)\n"
     ]
    }
   ],
   "source": [
    "# Number of images selected folder\n",
    "file_ct_train = 1349 + 3883\n",
    "file_ct_val = 8 + 8\n",
    "file_ct_test = 234 + 390\n",
    "\n",
    "# Run above-defined function for each of the datasets\n",
    "images, labels = import_data(path_train, file_ct_train)\n",
    "images_val, labels_val = import_data(path_val, file_ct_val)\n",
    "images_test, labels_test = import_data(path_test, file_ct_test)\n",
    "\n",
    "# Aggregate all datasets into one for inputs and one for outputs\n",
    "images = np.concatenate([images, images_val, images_test], axis=0)\n",
    "labels = np.concatenate([labels, labels_val, labels_test], axis=0)\n",
    "\n",
    "# Remove unnecessary data since memory issues can occur while running this script\n",
    "del images_val, labels_val\n",
    "del images_test, labels_test\n",
    "\n",
    "# Enhance data understanding. Verify all images are imported as would be expected\n",
    "print('Image data shape:', images.shape)\n",
    "print('Label data shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform a train-test split on all data\n",
    "\n",
    "As mentioned above, there is reason to believe that the splits provided by the dataset publishers may be flawed. As a result, the below cell creates a split into training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:03:34.056452Z",
     "iopub.status.busy": "2021-07-02T23:03:34.056195Z",
     "iopub.status.idle": "2021-07-02T23:03:34.690408Z",
     "shell.execute_reply": "2021-07-02T23:03:34.689430Z",
     "shell.execute_reply.started": "2021-07-02T23:03:34.056427Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-91eace7447d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Classic train-test split for TEST data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m x_train, x_test, y_train, y_test = train_test_split(images, labels, \n\u001b[0m\u001b[0;32m      3\u001b[0m                                                   \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRANDOM_STATE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                   test_size=VAL_SPLIT)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2197\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2199\u001b[1;33m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0m\u001b[0;32m   2200\u001b[0m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0;32m   2201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2197\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2199\u001b[1;33m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0m\u001b[0;32m   2200\u001b[0m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0;32m   2201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Classic train-test split for TEST data\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, \n",
    "                                                  random_state=RANDOM_STATE, \n",
    "                                                  test_size=VAL_SPLIT)\n",
    "\n",
    "# Classic train-test split for VALIDATION data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  random_state=RANDOM_STATE, \n",
    "                                                  test_size=VAL_SPLIT)\n",
    "\n",
    "\n",
    "# Remove unnecessary data since memory issues can occur while running this script\n",
    "del images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Function definitions\n",
    "\n",
    "Now that data has been imported, split and analyzed, the notebook soon proceeds to the modeling stage. However, first, valuable functions are defined to enable easier, consistent analysis of subsequent model iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define function to plot the performance of selected model across epoch iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:03:34.693009Z",
     "iopub.status.busy": "2021-07-02T23:03:34.692417Z",
     "iopub.status.idle": "2021-07-02T23:03:34.701935Z",
     "shell.execute_reply": "2021-07-02T23:03:34.700468Z",
     "shell.execute_reply.started": "2021-07-02T23:03:34.692970Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(history, metrics=['accuracy'], val=True):\n",
    "    \n",
    "    # Create one plot for each metric being analyzed\n",
    "    for metric in metrics:\n",
    "        \n",
    "        # Normalize casing to reduce possible errors\n",
    "        metric=str.lower(metric)\n",
    "        \n",
    "        # Define 'x' variable according to the number of epochs\n",
    "        x = range(len(history[metric]))\n",
    "\n",
    "        # Create figure and plot the metric\n",
    "        plt.figure()\n",
    "        plt.plot(x, history[metric], label='Train')\n",
    "\n",
    "        # If a validation split was used, plot its performance\n",
    "        if val == True:\n",
    "            plt.plot(x, history['val_'+metric], label='Validation')\n",
    "        \n",
    "        # Basic figure improvements\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.title(metric)\n",
    "        plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define function to plot the confusion matrices for selected models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:03:34.705117Z",
     "iopub.status.busy": "2021-07-02T23:03:34.704473Z",
     "iopub.status.idle": "2021-07-02T23:03:34.719485Z",
     "shell.execute_reply": "2021-07-02T23:03:34.718181Z",
     "shell.execute_reply.started": "2021-07-02T23:03:34.705068Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(model, images, labels):\n",
    "    # Reformat the \"Labels\" data to enable ease of use of Confusion Matrix\n",
    "    truth = (labels[:, 1]==1).astype('int')\n",
    "    \n",
    "    # Predict Labels using the inputted model\n",
    "    preds = np.argmax(model.predict(images), axis=-1)\n",
    "\n",
    "    # Instantiate confusion matrix, NOT normalized\n",
    "    matrix = confusion_matrix(truth, preds)#, normalize='true')\n",
    "    ConfusionMatrixDisplay(matrix).plot()\n",
    "    plt.xticks(ticks = [0, 1], labels=['No pneumonia', 'Pneumonia'])\n",
    "    plt.yticks(ticks = [0, 1], labels=['No pneumonia', 'Pneumonia'])\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.title('Correctness of Model Predictions (COUNT)');\n",
    "\n",
    "    # Instantiate confusion matrix, NORMALIZED\n",
    "    matrix_norm = confusion_matrix(truth, preds, normalize='true')\n",
    "    ConfusionMatrixDisplay(matrix_norm).plot()\n",
    "    plt.xticks(ticks = [0, 1], labels=['No pneumonia', 'Pneumonia'])\n",
    "    plt.yticks(ticks = [0, 1], labels=['No pneumonia', 'Pneumonia'])\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.title('Correctness of Model Predictions (%)')\n",
    "    \n",
    "    return matrix, matrix_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define top-level function to aggregate the previously-defined two functions, in addition to printing several performance metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:03:34.721676Z",
     "iopub.status.busy": "2021-07-02T23:03:34.721002Z",
     "iopub.status.idle": "2021-07-02T23:03:34.734072Z",
     "shell.execute_reply": "2021-07-02T23:03:34.733035Z",
     "shell.execute_reply.started": "2021-07-02T23:03:34.721614Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_pre_rec_f1(truth, pred):\n",
    "    print('Accuracy:', round(accuracy_score(truth, preds), 4)*100, \"%\")\n",
    "    print('Precision:', round(precision_score(truth, preds), 4)*100, \"%\")\n",
    "    print('Recall:', round(recall_score(truth, preds), 4)*100, \"%\")\n",
    "    print('F1 score:', round(f1_score(truth, preds), 4)*100, \"%\");\n",
    "\n",
    "\n",
    "\n",
    "def show_model_performance(model, x_val, y_val, fit=False):\n",
    "    \n",
    "    if fit != False:\n",
    "        plot_metrics(fit.history)\n",
    "    \n",
    "    show_confusion_matrix(model, x_val, y_val)\n",
    "    display(model.summary())\n",
    "    \n",
    "    # Reformat the \"Labels\" data to enable ease of use of Confusion Matrix\n",
    "    truth = (y_val[:, 1]==1).astype('int')\n",
    "    \n",
    "    # Predict Labels using the inputted model\n",
    "    preds = np.argmax(model.predict(x_val), axis=-1)\n",
    "\n",
    "    # Print performance metrics\n",
    "    acc_pre_rec_f1(truth, preds);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate and define callbacks and commonly used variables\n",
    "\n",
    "#### Callbacks\n",
    "- The EarlyStopping callback to eliminate the need for tuning the number of epochs used in a model. This way, the model can run until it has reached its maximum performance without wasting computational power. \n",
    "- The ModelCheckpoint callback is used to select the top-performing model over the full range of epochs used for each model training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:03:34.736464Z",
     "iopub.status.busy": "2021-07-02T23:03:34.735849Z",
     "iopub.status.idle": "2021-07-02T23:03:34.743459Z",
     "shell.execute_reply": "2021-07-02T23:03:34.742503Z",
     "shell.execute_reply.started": "2021-07-02T23:03:34.736416Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input shape\n",
    "- Define the shape of the input data that will be used throughout each of the subsequent models based on the resolution of the downsampled input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:03:34.745675Z",
     "iopub.status.busy": "2021-07-02T23:03:34.745066Z",
     "iopub.status.idle": "2021-07-02T23:03:34.755875Z",
     "shell.execute_reply": "2021-07-02T23:03:34.754594Z",
     "shell.execute_reply.started": "2021-07-02T23:03:34.745641Z"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (TARGET_SIZE, TARGET_SIZE, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Modeling\n",
    "\n",
    "Using the Keras *Sequential* model, the below models investigate combinations of various layer combinations, node counts, and normalizations to find the best performance. Note that for the purpose of this analysis, there was insufficient computational power to perform extensive grid searches of hyperparameters, as would be needed to fully optimize each model. Instead, research-backed estimates of high-performing hyperparameter combinations are made. Future improvements would include exhaustive hyperparameter optimization, but that will be left for future analyses with more computational resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Baseline Model\n",
    "The baseline performance shows the performance of a *random guess* methodology, which can be helpful when first developing a model. It allows the question to be answered, *does the model actually provide any value?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(y):\n",
    "    model = lambda x: np.random.choice([0, 1])\n",
    "    preds_map = map(model_baseline, y_val[:, 1])\n",
    "    preds = np.array(list(preds_map))\n",
    "    return preds\n",
    "\n",
    "preds = baseline_model(y_val[:, 1])\n",
    "truth = y_val[:, 1]\n",
    "\n",
    "acc_baseline = round(accuracy_score(truth, preds)*100, 2)\n",
    "prec_baseline = round(precision_score(truth, preds)*100, 2)\n",
    "rec_baseline = round(recall_score(truth, preds)*100, 2)\n",
    "f1_baseline = round(f1_score(truth, preds)*100, 2)\n",
    "\n",
    "print('\\nBASELINE performance on validation data:')\n",
    "print('-------------------------------')\n",
    "print('Accuracy:', acc_baseline, \"%\")\n",
    "print('Precision:', prec_baseline, \"%\")\n",
    "print('Recall:', rec_baseline, \"%\")\n",
    "print('F1 score:', f1_baseline, \"%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The First Simple Model\n",
    "Define the FSM using only one Dense layer with two nodes. This model flattens the input data and subsequently applies the one additional layer within nearly hyperparameters set to the default.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:03:34.762305Z",
     "iopub.status.busy": "2021-07-02T23:03:34.760936Z",
     "iopub.status.idle": "2021-07-02T23:03:52.756860Z",
     "shell.execute_reply": "2021-07-02T23:03:52.756082Z",
     "shell.execute_reply.started": "2021-07-02T23:03:34.762267Z"
    }
   },
   "outputs": [],
   "source": [
    "model_fsm = Sequential()\n",
    "model_fsm.add(Flatten())\n",
    "model_fsm.add(Dense(2, activation='softmax', \n",
    "             input_shape=input_shape))\n",
    "\n",
    "model_fsm.compile(optimizer='SGD', \n",
    "                  metrics=['accuracy'], \n",
    "                  loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "path_model = 'tmp/model_fsm.h5'\n",
    "\n",
    "if os.path.exists(path_model):\n",
    "    os.remove(path_model)\n",
    "check = ModelCheckpoint(path_model,save_best_only=True)\n",
    "\n",
    "fit_fsm = model_fsm.fit(x=x_train, \n",
    "                        y=y_train, \n",
    "                        epochs=500,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[early_stopper, check],\n",
    "                        verbose=0)\n",
    "\n",
    "\n",
    "model_fsm.load_weights(path_model)\n",
    "\n",
    "show_model_performance(model_fsm, x_val, y_val, fit_fsm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add convolutional layer to previous model\n",
    "\n",
    "Given an awareness that Convolutional Neural Networks (CNNs) are often top performing models for image classification, the below model adds a 2D convolutional layer to the previous model. The node count, kernel size and activation function were based on simple testing. Recall that there was insufficient computational power to perform grid searches of hyperparameters, as would be needed to fully optimize each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:03:52.758893Z",
     "iopub.status.busy": "2021-07-02T23:03:52.758556Z",
     "iopub.status.idle": "2021-07-02T23:05:18.578780Z",
     "shell.execute_reply": "2021-07-02T23:05:18.578009Z",
     "shell.execute_reply.started": "2021-07-02T23:03:52.758856Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, activation='relu',\n",
    "                 kernel_size=7,  \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              metrics=['accuracy'], \n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "path_model = 'tmp/model_2.h5'\n",
    "if os.path.exists(path_model):\n",
    "    os.remove(path_model)\n",
    "check = ModelCheckpoint(path_model,save_best_only=True)\n",
    "\n",
    "fit = model.fit(x=x_train, \n",
    "                y=y_train,\n",
    "                epochs=500,\n",
    "                callbacks=[early_stopper, check],\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0)\n",
    "\n",
    "model.load_weights(path_model)\n",
    "\n",
    "show_model_performance(model, x_val, y_val, fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Increase node count substantially\n",
    "Looking at the above accuracy plots over a range of epochs, the is clear that the model has a relatively low accuracy relative to a wide range of testing in the EDA phase of this project. Accordingly, additional nodes are added to the Convolutional layer of the below model to create a more tunable model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:05:18.580307Z",
     "iopub.status.busy": "2021-07-02T23:05:18.579986Z",
     "iopub.status.idle": "2021-07-02T23:10:00.246045Z",
     "shell.execute_reply": "2021-07-02T23:10:00.245077Z",
     "shell.execute_reply.started": "2021-07-02T23:05:18.580272Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, activation='relu',\n",
    "                 kernel_size=11,  \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              metrics=['accuracy'], \n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "path_model = 'tmp/model_3.h5'\n",
    "if os.path.exists(path_model):\n",
    "    os.remove(path_model)\n",
    "check = ModelCheckpoint(path_model,save_best_only=True)\n",
    "\n",
    "fit = model.fit(x=x_train, \n",
    "                y=y_train,\n",
    "                epochs=500,\n",
    "                callbacks=[early_stopper, check],\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0)\n",
    "\n",
    "model.load_weights(path_model)\n",
    "\n",
    "show_model_performance(model, x_val, y_val, fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instead, increase depth with pooling\n",
    "As we can see, the accuracy actually decreased when adding extensive additional nodes. Instead of increasing the node count, we will return to a lower node count, and instead add an additional Convolutional layer, along with Pooling after each of the Convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:10:00.248074Z",
     "iopub.status.busy": "2021-07-02T23:10:00.247681Z",
     "iopub.status.idle": "2021-07-02T23:11:07.674114Z",
     "shell.execute_reply": "2021-07-02T23:11:07.673350Z",
     "shell.execute_reply.started": "2021-07-02T23:10:00.248038Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, activation='relu',\n",
    "                 kernel_size=7,  \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(16, activation='relu',\n",
    "                 kernel_size=7,\n",
    "                 padding='same'))\n",
    "          \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              metrics=['accuracy'], \n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "path_model = 'tmp/model_4.h5'\n",
    "if os.path.exists(path_model):\n",
    "    os.remove(path_model)\n",
    "check = ModelCheckpoint(path_model,save_best_only=True)\n",
    "\n",
    "fit = model.fit(x=x_train, \n",
    "                y=y_train,\n",
    "                epochs=500,\n",
    "                callbacks=[early_stopper, check],\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0)\n",
    "\n",
    "model.load_weights(path_model)\n",
    "\n",
    "show_model_performance(model, x_val, y_val, fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add one more Convolutional layer\n",
    "After seeing that the above additon of a layer increased all performance metrics, in addition to reducing overfitting, the below model takes it one step further by adding another Convolutional layer to investigate the impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:11:07.675886Z",
     "iopub.status.busy": "2021-07-02T23:11:07.675520Z",
     "iopub.status.idle": "2021-07-02T23:12:38.680199Z",
     "shell.execute_reply": "2021-07-02T23:12:38.679332Z",
     "shell.execute_reply.started": "2021-07-02T23:11:07.675850Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, activation='relu',\n",
    "                 kernel_size=7,  \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(16, activation='relu',\n",
    "                 kernel_size=7, \n",
    "                 padding='same'))\n",
    "          \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, activation='relu',\n",
    "                 kernel_size=7,  \n",
    "                 padding='same'))\n",
    "          \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              metrics=['accuracy'], \n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "path_model = 'tmp/model_5.h5'\n",
    "if os.path.exists(path_model):\n",
    "    os.remove(path_model)\n",
    "check = ModelCheckpoint(path_model,save_best_only=True)\n",
    "\n",
    "fit = model.fit(x=x_train, \n",
    "                y=y_train,\n",
    "                epochs=500,\n",
    "                callbacks=[early_stopper, check],\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0)\n",
    "\n",
    "model.load_weights(path_model)\n",
    "\n",
    "show_model_performance(model, x_val, y_val, fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Regularization to top performing Convolutional\n",
    "\n",
    "The additional layer above added accuracy and increased the F1 score relative to the previous model. However, it is still clear that there is overfitting. Regularization is a common solution for addressing overfitting. Accordingly, the below model adds L2 regularization to each Convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:12:38.682072Z",
     "iopub.status.busy": "2021-07-02T23:12:38.681736Z",
     "iopub.status.idle": "2021-07-02T23:14:29.533364Z",
     "shell.execute_reply": "2021-07-02T23:14:29.532631Z",
     "shell.execute_reply.started": "2021-07-02T23:12:38.682039Z"
    }
   },
   "outputs": [],
   "source": [
    "LAMBDA = 0.005\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, activation='relu',\n",
    "                 kernel_size=7,  \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(LAMBDA)))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(16, activation='relu',\n",
    "                 kernel_size=7,  \n",
    "                 padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(LAMBDA)))\n",
    "          \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, activation='relu',\n",
    "                 kernel_size=7,  \n",
    "                 padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(LAMBDA)))\n",
    "          \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              metrics=['accuracy'], \n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "path_model = 'tmp/model_6.h5'\n",
    "if os.path.exists(path_model):\n",
    "    os.remove(path_model)\n",
    "check = ModelCheckpoint(path_model,save_best_only=True)\n",
    "\n",
    "fit = model.fit(x=x_train, \n",
    "                y=y_train,\n",
    "                epochs=500,\n",
    "                callbacks=[early_stopper, check],\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0)\n",
    "\n",
    "model.load_weights(path_model)\n",
    "\n",
    "show_model_performance(model, x_val, y_val, fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Dropout to top Convolutional\n",
    "The performance of the above model didn't change substantially with the additional of regularization. The below model will add Dropout layers after each pooling layer to see if they can increase performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:14:29.535450Z",
     "iopub.status.busy": "2021-07-02T23:14:29.535121Z",
     "iopub.status.idle": "2021-07-02T23:20:40.645446Z",
     "shell.execute_reply": "2021-07-02T23:20:40.644674Z",
     "shell.execute_reply.started": "2021-07-02T23:14:29.535414Z"
    }
   },
   "outputs": [],
   "source": [
    "DROPOUT = 0.3\n",
    "LAMBDA = 0.005\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, activation='relu',\n",
    "                 kernel_size=7,  \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(LAMBDA)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Conv2D(16, activation='relu',\n",
    "                 kernel_size=7,\n",
    "                 padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(LAMBDA)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Conv2D(32, activation='relu',\n",
    "                 kernel_size=7,  \n",
    "                 padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(LAMBDA)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              metrics=['accuracy'], \n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "path_model = 'tmp/model_7.h5'\n",
    "if os.path.exists(path_model):\n",
    "    os.remove(path_model)\n",
    "check = ModelCheckpoint(path_model,save_best_only=True)\n",
    "\n",
    "fit = model.fit(x=x_train, \n",
    "                y=y_train,\n",
    "                epochs=500,\n",
    "                callbacks=[early_stopper, check],\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0)\n",
    "\n",
    "model.load_weights(path_model)\n",
    "\n",
    "show_model_performance(model, x_val, y_val, fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The top performer\n",
    "After looking at each of the printed performance metrics for the above model, it appears to perform better than all predecessors. It is accordingly selected as the top performing model and will be applied to the Test dataset below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:20:40.647167Z",
     "iopub.status.busy": "2021-07-02T23:20:40.646844Z",
     "iopub.status.idle": "2021-07-02T23:20:40.653110Z",
     "shell.execute_reply": "2021-07-02T23:20:40.652324Z",
     "shell.execute_reply.started": "2021-07-02T23:20:40.647134Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the model and fit from the preceding model since it was\n",
    "# determined to be the best model\n",
    "model_top = model\n",
    "fit_top = fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Evaluation\n",
    "Now that the top performing model has been selected, it will be retrained using both *train* and *validation* datasets before being tested on the withheld, previously untouched *test* dataset. Note that this same process is repeated for the FSM in order to compare performance improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train FSM on full dataset (Training and Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:20:40.654778Z",
     "iopub.status.busy": "2021-07-02T23:20:40.654238Z",
     "iopub.status.idle": "2021-07-02T23:20:58.264909Z",
     "shell.execute_reply": "2021-07-02T23:20:58.264038Z",
     "shell.execute_reply.started": "2021-07-02T23:20:40.654722Z"
    }
   },
   "outputs": [],
   "source": [
    "path_model = 'tmp/final_model_fsm.h5'\n",
    "if os.path.exists(path_model):\n",
    "    os.remove(path_model)\n",
    "check = ModelCheckpoint(path_model,save_best_only=True)\n",
    "\n",
    "fit_fsm = model_fsm.fit(x=np.concatenate([x_train, x_val], axis=0), \n",
    "                y=np.concatenate([y_train, y_val], axis=0),\n",
    "                epochs=500,\n",
    "                callbacks=[early_stopper, check],\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0)\n",
    "\n",
    "model_fsm.load_weights(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Top Model on full dataset (Training and Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:20:58.266430Z",
     "iopub.status.busy": "2021-07-02T23:20:58.266121Z",
     "iopub.status.idle": "2021-07-02T23:27:15.913465Z",
     "shell.execute_reply": "2021-07-02T23:27:15.912603Z",
     "shell.execute_reply.started": "2021-07-02T23:20:58.266397Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_model = 'tmp/final_model_top.h5'\n",
    "if os.path.exists(path_model):\n",
    "    os.remove(path_model)\n",
    "check = ModelCheckpoint(path_model,save_best_only=True)\n",
    "\n",
    "fit_top = model_top.fit(x=np.concatenate([x_train, x_val], axis=0), \n",
    "                y=np.concatenate([y_train, y_val], axis=0),\n",
    "                epochs=500,\n",
    "                callbacks=[early_stopper, check],\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0)\n",
    "\n",
    "model_top.load_weights(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure performance on Test data\n",
    "Now that the FSM and top model are fully trained, they are evaluated below on the *test* data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Simple Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:27:15.915058Z",
     "iopub.status.busy": "2021-07-02T23:27:15.914729Z",
     "iopub.status.idle": "2021-07-02T23:27:16.957852Z",
     "shell.execute_reply": "2021-07-02T23:27:16.956905Z",
     "shell.execute_reply.started": "2021-07-02T23:27:15.915025Z"
    }
   },
   "outputs": [],
   "source": [
    "show_model_performance(model_fsm, x_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T23:27:16.959471Z",
     "iopub.status.busy": "2021-07-02T23:27:16.959114Z",
     "iopub.status.idle": "2021-07-02T23:27:18.974965Z",
     "shell.execute_reply": "2021-07-02T23:27:18.974197Z",
     "shell.execute_reply.started": "2021-07-02T23:27:16.959434Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONFUSION MATRIX FOR TEST DATA, ***FSM***\n",
    "show_model_performance(model_top, x_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Conclusion\n",
    "Below is a comparison of the performance metrics for the baseline, first-simple and final models. \n",
    "\n",
    "#### Baseline performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-051d036df4a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtruth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Precision:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Recall:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'baseline_model' is not defined"
     ]
    }
   ],
   "source": [
    "preds = baseline_model(y_test[:, 1])\n",
    "truth = y_test[:, 1]\n",
    "acc_pre_rec_f1(truth, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First simple model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(model_fsm.predict(x_test), axis=-1)\n",
    "truth = (y_val[:, 1]==1).astype('int')\n",
    "acc_pre_rec_f1(truth, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(model_top.predict(x_test), axis=-1)\n",
    "truth = (y_val[:, 1]==1).astype('int')\n",
    "acc_pre_rec_f1(truth, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is noteworthy improvement across the model development process. The best performing model does in fact enable results that are non-trivially better than the first simple model. However, it is also worth noting that for an application where speed was prioritized over performance, the first simple model is sufficiently viable. The training time was substantially lower, giving it merit of its own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future improvements\n",
    "Although the above models are high performers, there is always room for improvement or further exploration. Some of those possibilities are discussed below.\n",
    "\n",
    "- **Increase image resolution:** currently, photos are downsampled from roughly 800x1200 pixels to roughly 200x200 pixels. This choice was made to enable the script to run without crashing due to memory shortages. This issue is inherent to the methodology used to the images being stored in memory as a numpy array. However, analyzing these images in \"batches\" would eliminate the need to store an array in memory. As a result, more data could be analyzed. With more data, presumably the model could be more effective. \n",
    "- **Hyperparameter grid searches:** currently, ideal hyperparameters were selected based on some fine tuning and outside research. Note that not all iterations of those models are included in this notebook so that the notebook can run in a reasomnable amount of time. An exhaustive hyperparameter optimization is necessary to truly identify the top performing model, but that will be left for future analyses with more computational resources. \n",
    "- Expansion to further x-ray image recognition datasets provided by the same academic researchers who provided this dataset (see README for citation). Testing these models on additional datasets could be an interesting project, and could provide insight regarding the broad applicability of the models at hand. \n",
    "\n",
    "### Citations\n",
    "(1) Kermany D, Goldbaum M, Cai W et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell. 2018; 172(5):1122-1131. doi:10.1016/j.cell.2018.02.010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
